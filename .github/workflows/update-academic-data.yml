name: Update Academic Data

on:
  schedule:
    # æ¯ä¸ªå·¥ä½œæ—¥ï¼ˆå‘¨ä¸€è‡³å‘¨äº”ï¼‰åŒ—äº¬æ—¶é—´æ—©ä¸Š8:00è¿è¡Œï¼ˆUTCæ—¶é—´0:00ï¼‰
    # æ ¼å¼: åˆ† æ—¶ æ—¥ æœˆ å‘¨å‡  (0-6ï¼Œ0æ˜¯å‘¨æ—¥)
    - cron: '0 0 * * 1-5'
  # å…è®¸æ‰‹åŠ¨è§¦å‘
  workflow_dispatch:

jobs:
  update-data:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '16'

      - name: Install dependencies
        run: npm install node-fetch@2 dotenv

      - name: Create updater script
        run: |
          cat > update-data.js << 'EOL'
          require('dotenv').config();
          const fetch = require('node-fetch');

          // --- è°ƒè¯•æ¨¡å¼å¼€å…³ ---
          // è®¾ç½®ä¸º true æ¥è·³è¿‡APIè°ƒç”¨ï¼Œä½¿ç”¨å·²æœ‰çš„ academic_data.json ä¸­çš„è®ºæ–‡æ•°æ®
          // è®¾ç½®ä¸º false ä»¥æ­£å¸¸è°ƒç”¨ Perplexity API
          const DEBUG_MODE = true; 

          // é…ç½®
          const GITHUB_TOKEN = process.env.GITHUB_TOKEN;
          const GITHUB_REPO = process.env.GITHUB_REPOSITORY;
          const DATA_FILE = 'academic_data.json';
          const PERPLEXITY_API_KEY = process.env.PERPLEXITY_API_KEY;
          const DEFAULT_KEYWORDS = ["é€šä¿¡", "AI 6G", "Agent", "LLM", "è¯­ä¹‰é€šä¿¡"];
          // English keyword mapping for better API results
          const KEYWORDS_ENGLISH_MAPPING = {
            "é€šä¿¡": "Communications",
            "AI 6G": "AI 6G",
            "Agent": "Agent",
            "LLM": "Large Language Model",
            "è¯­ä¹‰é€šä¿¡": "Semantic Communications"
          };

          // è·å–åŒ—äº¬æ—¶é—´
          function getBeijingTime() {
            const now = new Date();
            const utcOffset = now.getTimezoneOffset() * 60000;
            const utcTime = now.getTime() + utcOffset;
            const beijingTime = new Date(utcTime + (3600000 * 8)); 
            return beijingTime;
          }

          // ä»GitHubè·å–ç°æœ‰æ•°æ®
          async function fetchExistingData() {
            try {
              const url = `https://api.github.com/repos/${GITHUB_REPO}/contents/${DATA_FILE}`;
              const headers = { 'Authorization': `token ${GITHUB_TOKEN}` };
              
              const response = await fetch(url, { headers });
              
              if (!response.ok) {
                if (response.status === 404) {
                  console.log("Data file not found on GitHub. Will create it.");
                  return { keywords: DEFAULT_KEYWORDS, papers: [] };
                }
                throw new Error(`GitHub API error: ${response.status}`);
              }
              
              const data = await response.json();
              const content = Buffer.from(data.content, 'base64').toString('utf8');
              const parsedData = JSON.parse(content);
              console.log("Successfully fetched existing data from GitHub");
              return { ...parsedData, sha: data.sha };
            } catch (error) {
              console.error("Error fetching existing data:", error.message);
              return { keywords: DEFAULT_KEYWORDS, papers: [] };
            }
          }

          // è°ƒç”¨Perplexity APIè·å–è®ºæ–‡
          async function querySonarPro(question) {
            if (!PERPLEXITY_API_KEY) {
              throw new Error("Perplexity API key is missing");
            }
            
            const url = "https://api.perplexity.ai/chat/completions";
            const response = await fetch(url, {
              method: 'POST',
              headers: {
                'Authorization': `Bearer ${PERPLEXITY_API_KEY}`,
                'Content-Type': 'application/json'
              },
              body: JSON.stringify({
                model: "sonar-pro",
                messages: [{ role: "user", content: question }]
              })
            });
            
            if (!response.ok) {
              const errorText = await response.text();
              throw new Error(`Perplexity API error (${response.status}): ${errorText}`);
            }
            
            const result = await response.json();
            return {
              content: result.choices[0].message.content,
              citations: result.citations || []
            };
          }

          // è§£æAPIå“åº”ä¸ºè®ºæ–‡å¯¹è±¡
          function parsePaperInfo(content, citations, englishKeyword, originalKeyword) {
            // ç®€åŒ–çš„è§£æé€»è¾‘
            function extractField(text, labelStarts, nextLabelStarts = []) {
              const labelEndChars = ":ï¼š";
              for (const start of labelStarts) {
                for (const endChar of labelEndChars) {
                  const fullLabel = start + endChar;
                  let startIndex = text.toLowerCase().indexOf(fullLabel.toLowerCase());
                  if (startIndex === -1) continue;
                  
                  startIndex += fullLabel.length;
                  
                  let endIndex = text.length;
                  for (const nextStart of nextLabelStarts) {
                    for (const nextEndChar of labelEndChars) {
                      const nextFullLabel = nextStart + nextEndChar;
                      const tempEndIndex = text.toLowerCase().indexOf(nextFullLabel.toLowerCase(), startIndex);
                      if (tempEndIndex !== -1 && tempEndIndex < endIndex) {
                        endIndex = tempEndIndex;
                      }
                    }
                  }
                  
                  return text.substring(startIndex, endIndex).trim();
                }
              }
              return null;
            }
            
            try {
              const paper = {
                id: `paper-api-${Date.now()}-${Math.random().toString(36).substring(7)}`,
                title: "æ ‡é¢˜æå–å¤±è´¥",
                url: "#",
                authors: ["ä½œè€…æå–å¤±è´¥"],
                affiliations: ["å•ä½æå–å¤±è´¥"],
                journal: "æœŸåˆŠ/ä¼šè®®æå–å¤±è´¥",
                publicationDate: `${getBeijingTime().getFullYear()}å¹´`,
                paperKeywords: ["è‡ªåŠ¨æ›´æ–°"], // å»æ‰originalKeywordï¼ˆå°±æ˜¯é‚£ä¸ªé•¿çš„å¹¶åœ¨ä¸€èµ·çš„å…³é”®è¯ï¼‰
                snippet: "æ‘˜è¦æœªèƒ½æå–ã€‚",
                interpretation: "æ·±å…¥è§£è¯»æœªèƒ½æå–ã€‚"
              };
              
              const allLabels = ["title", "æ ‡é¢˜", "authors", "ä½œè€…", "affiliations", "å•ä½", 
                                "journal/conference", "æœŸåˆŠ/ä¼šè®®", "publication year", "å‘è¡¨å¹´ä»½", 
                                "url", "abstract", "æ‘˜è¦", "in-depth analysis", "æ·±å…¥è§£è¯»", "keywords", "å…³é”®è¯"];
              
              const title = extractField(content, ["title", "æ ‡é¢˜"], 
                allLabels.filter(l => !["title", "æ ‡é¢˜"].includes(l.toLowerCase())));
              if (title) paper.title = title;
              
              const authorsText = extractField(content, ["authors", "ä½œè€…"], 
                allLabels.filter(l => !["authors", "ä½œè€…"].includes(l.toLowerCase())));
              if (authorsText) paper.authors = authorsText.split(/,|ã€|and/i).map(a => a.trim()).filter(a => a.length > 0);
              
              const affiliationsText = extractField(content, ["affiliations", "å•ä½"], 
                allLabels.filter(l => !["affiliations", "å•ä½"].includes(l.toLowerCase())));
              if (affiliationsText) paper.affiliations = affiliationsText.split(/;/).map(a => a.trim()).filter(a => a.length > 0);
              
              const journal = extractField(content, ["journal/conference", "æœŸåˆŠ/ä¼šè®®"], 
                allLabels.filter(l => !["journal/conference", "æœŸåˆŠ/ä¼šè®®"].includes(l.toLowerCase())));
              if (journal) paper.journal = journal;
              
              const publicationDate = extractField(content, ["publication year", "å‘è¡¨å¹´ä»½"], 
                allLabels.filter(l => !["publication year", "å‘è¡¨å¹´ä»½"].includes(l.toLowerCase())));
              if (publicationDate) paper.publicationDate = publicationDate;
              
              const urlText = extractField(content, ["url"], 
                allLabels.filter(l => !["url"].includes(l.toLowerCase())));
              if (urlText && urlText.startsWith("http")) paper.url = urlText;
              else if (citations && citations.length > 0) {
                const potentialUrl = citations.find(c => c.url && 
                  (c.url.includes('arxiv.org') || c.url.includes('doi.org') || 
                   c.url.includes('acm.org') || c.url.includes('ieee.org')))?.url;
                if (potentialUrl) paper.url = potentialUrl;
                else if (citations[0].url) paper.url = citations[0].url;
              }
              
              const snippet = extractField(content, ["abstract", "æ‘˜è¦"], 
                allLabels.filter(l => !["abstract", "æ‘˜è¦"].includes(l.toLowerCase())));
              if (snippet) paper.snippet = snippet;
              
              const interpretation = extractField(content, ["in-depth analysis", "æ·±å…¥è§£è¯»ä¸åˆ†æ", "æ·±å…¥è§£è¯»"], 
                allLabels.filter(l => !["in-depth analysis", "æ·±å…¥è§£è¯»ä¸åˆ†æ", "æ·±å…¥è§£è¯»"].includes(l.toLowerCase())));
              if (interpretation) paper.interpretation = interpretation;
              
              const keywordsText = extractField(content, ["keywords", "å…³é”®è¯"]);
              if (keywordsText) {
                paper.paperKeywords = [...keywordsText.split(/,|ã€/).map(k => k.trim()).filter(k => k.length > 0)]; // <--- ä¿®æ”¹åï¼Œç§»é™¤äº† originalKeyword
              }
              
              // éªŒè¯å¿…è¦å­—æ®µæ˜¯å¦æœ‰æ•ˆ
              if (paper.title === "æ ‡é¢˜æå–å¤±è´¥" || paper.snippet === "æ‘˜è¦æœªèƒ½æå–ã€‚") {
                console.log(`è§£æå¤±è´¥: å…³é”®è¯ "${originalKeyword}" çš„è®ºæ–‡æ ‡é¢˜æˆ–æ‘˜è¦æå–å¤±è´¥`);
                return null;
              }
              
              return paper;
            } catch (error) {
              console.error(`è§£æå…³é”®è¯ "${originalKeyword}" çš„å†…å®¹æ—¶å‡ºé”™:`, error);
              return null;
            }
          }

          // è·å–æ‰€æœ‰è®ºæ–‡
          async function fetchPapers(keywords) {
            const fetchedPapers = [];
            
            if (keywords.length === 0) {
              console.log("No keywords provided for search.");
              return fetchedPapers;
            }

            try {
              const combinedKeywords = keywords.join(', ');
              const englishKeywords = keywords.map(keyword => KEYWORDS_ENGLISH_MAPPING[keyword] || keyword);
              const combinedEnglishKeywords = englishKeywords.join(', ');
              
              const maxPapersToFetch = 5;
              
              console.log(`æ­£åœ¨ä¸ºç»„åˆå…³é”®è¯ "${combinedKeywords}" è·å–è®ºæ–‡...`);
              console.log(`ä½¿ç”¨è‹±æ–‡å…³é”®è¯: "${combinedEnglishKeywords}"`);
              
              const question = `
              Please find ${maxPapersToFetch} highly relevant research papers that are related to the following keywords as a whole: "${combinedEnglishKeywords}". Focus on papers published in the last 12-18 months.
              
              For each paper, provide the information using the following exact format:
              
              PAPER 1:
              Title: [The full title of the paper]
              Authors: [List of authors, separated by commas. Example: John Doe, Jane Smith]
              Affiliations: [List of main affiliations, separated by semicolons. Example: University A; Organization B]
              Journal/Conference: [Name of the journal or conference]
              Publication Year: [Year, e.g., 2024]
              URL: [A direct, accessible URL to the paper (e.g., ArXiv, DOI, official publisher page)]
              Abstract: [è¯·ç”¨ä¸­æ–‡æä¾›è¯¦ç»†çš„è®ºæ–‡æ‘˜è¦ï¼Œå¤§çº¦150-250å­—ã€‚]
              In-depth Analysis: [è¯·ç”¨ä¸­æ–‡å¯¹è®ºæ–‡çš„é‡è¦æ€§ã€åˆ›æ–°ç‚¹ã€æ½œåœ¨å½±å“å’Œå±€é™æ€§è¿›è¡Œæ·±å…¥è§£è¯»ä¸åˆ†æï¼Œå¤§çº¦200-300å­—ã€‚]
              Keywords: [5-7 relevant keywords for this paper, separated by commas]
              
              PAPER 2:
              Title: ...
              [repeat format for each paper]
              
              It's very important that each paper is related to multiple keywords rather than just focusing on one keyword. The papers should be at the intersection of as many of these areas as possible: ${englishKeywords.join(', ')}
              `;
              
              const { content, citations } = await querySonarPro(question);
              
              const paperSections = content.split(/PAPER \d+:/g).filter(section => section.trim().length > 0);
              
              if (paperSections.length === 0) {
                const paperInfo = parsePaperInfo(content, citations, combinedEnglishKeywords, combinedKeywords);
                if (paperInfo) {
                  fetchedPapers.push(paperInfo);
                  console.log(`æˆåŠŸè·å–ç»„åˆå…³é”®è¯çš„è®ºæ–‡: ${paperInfo.title}`);
                }
              } else {
                for (let i = 0; i < Math.min(paperSections.length, maxPapersToFetch); i++) {
                  const section = paperSections[i];
                  const paperInfo = parsePaperInfo(section, citations, combinedEnglishKeywords, combinedKeywords);
                  
                  if (paperInfo) {
                    fetchedPapers.push(paperInfo);
                    console.log(`æˆåŠŸè·å–ç¬¬ ${i+1} ç¯‡è®ºæ–‡: ${paperInfo.title}`);
                  } else {
                    console.warn(`æ— æ³•ä»ç¬¬ ${i+1} éƒ¨åˆ†æå–æœ‰æ•ˆçš„è®ºæ–‡ä¿¡æ¯`);
                  }
                }
              }
            } catch (error) {
              console.error(`å¤„ç†ç»„åˆå…³é”®è¯æ—¶å‡ºé”™:`, error);
            }
            
            return fetchedPapers;
          }

          // ä¿å­˜æ•°æ®åˆ°GitHub
          async function saveToGitHub(data, sha) {
            try {
              const url = `https://api.github.com/repos/${GITHUB_REPO}/contents/${DATA_FILE}`;
              const headers = {
                'Authorization': `token ${GITHUB_TOKEN}`,
                'Content-Type': 'application/json'
              };
              
              const content = JSON.stringify(data, null, 2);
              const encodedContent = Buffer.from(content).toString('base64');
              
              const body = {
                message: `æ›´æ–°å­¦æœ¯æ•°æ® - ${new Date().toISOString()}`,
                content: encodedContent,
                branch: 'main'
              };
              
              if (sha) body.sha = sha;
              
              const response = await fetch(url, {
                method: 'PUT',
                headers,
                body: JSON.stringify(body)
              });
              
              if (!response.ok) {
                const errorText = await response.text();
                throw new Error(`ä¿å­˜åˆ°GitHubå¤±è´¥ (${response.status}): ${errorText}`);
              }
              
              console.log("æˆåŠŸä¿å­˜æ•°æ®åˆ°GitHub");
            } catch (error) {
              console.error("ä¿å­˜æ•°æ®åˆ°GitHubæ—¶å‡ºé”™:", error);
              throw error;
            }
          }

          

          // ä¸»å‡½æ•°
          async function main() {
            try {
              // --- æ­¥éª¤ 1: è·å–ç°æœ‰æ•°æ®å’Œå·²å­˜åœ¨çš„è®ºæ–‡URL ---
              const existingData = await fetchExistingData();

              // --- æ–°å¢ï¼šå¤„ç†æ‰‹åŠ¨å’Œçƒ­é—¨å…³é”®è¯ ---
              // æ£€æŸ¥æ˜¯å¦ä¸ºæ–°æ•°æ®ç»“æ„ï¼Œå¹¶æä¾›å‘åå…¼å®¹æ€§
              const isNewStructure = existingData.keywords && typeof existingData.keywords === 'object' && !Array.isArray(existingData.keywords);
              const manualKeywords = isNewStructure ? existingData.keywords.manual : (existingData.keywords || DEFAULT_KEYWORDS);
              const hotKeywords = isNewStructure ? existingData.keywords.hot : [];

              // ä½¿ç”¨æ‰‹åŠ¨å’Œçƒ­é—¨å…³é”®è¯çš„å¹¶é›†è¿›è¡Œæœ¬æ¬¡æœç´¢
              const searchKeywords = Array.from(new Set([...manualKeywords, ...hotKeywords]));
              // --- ä¿®æ”¹ç»“æŸ ---

              
              const existingPaperUrls = new Set((existingData.papers || []).map(p => p.url).filter(url => url && url !== "#"));
              
              console.log(`å°†ä½¿ç”¨ä»¥ä¸‹å…³é”®è¯: ${searchKeywords.join(', ')}`);
              console.log(`ç›®å‰å·²è®°å½• ${existingPaperUrls.size} ç¯‡è®ºæ–‡çš„URLã€‚`);
              
              // (è°ƒè¯•æ¨¡å¼é€»è¾‘ï¼Œä¿æŒä¸å˜)
              let papers;
              if (DEBUG_MODE) {
                console.log("ğŸš€ è°ƒè¯•æ¨¡å¼å·²å¼€å¯ï¼Œå°†ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡ Perplexity API è°ƒç”¨ã€‚");
                papers = existingData.papers || [];
              } else {
                console.log("è°ƒç”¨ Perplexity API è·å–æ–°è®ºæ–‡...");
                papers = await fetchPapers(searchKeywords);
              }
              
              if (papers.length === 0) {
                console.log("æœªèƒ½è·å–ä»»ä½•è®ºæ–‡æ•°æ®ï¼Œå·¥ä½œæµç»“æŸã€‚");
                return;
              }

              // --- æ­¥éª¤ 2 & 3: è¿‡æ»¤æ‰é‡å¤è®ºæ–‡ ---
              const newUniquePapers = papers.filter(p => !existingPaperUrls.has(p.url));

              // --- æ­¥éª¤ 4: å¦‚æœæ²¡æœ‰æ–°è®ºæ–‡åˆ™æå‰ç»“æŸ ---
              if (newUniquePapers.length === 0) {
                  console.log("æœ¬æ¬¡è·å–çš„è®ºæ–‡å‡å·²å­˜åœ¨ï¼Œæ²¡æœ‰æ–°å†…å®¹å¯æ›´æ–°ã€‚");
                  return;
              }
              console.log(`APIè¿”å›äº† ${papers.length} ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ ${newUniquePapers.length} ç¯‡æ˜¯å…¨æ–°çš„ã€‚`);

              // --- åç»­æ‰€æœ‰æ“ä½œï¼Œéƒ½åŸºäº newUniquePapers ---
              const allKeywords = newUniquePapers.flatMap(p => p.paperKeywords).filter(k => typeof k === 'string' && k.length > 0);

              const keywordFreq = allKeywords.reduce((acc, k) => {
                acc[k] = (acc[k] || 0) + 1;
                return acc;
              }, {});

              const sortedKeywordsFull = Object.entries(keywordFreq)
                .sort((a, b) => b[1] - a[1])
                .map(([k, count]) => ({ keyword: k, count }));

              // (saveKeywordsFullToGitHub å‡½æ•°å®šä¹‰ä¿æŒä¸å˜)
              async function saveKeywordsFullToGitHub(keywordsFullData) {
                try {
                  const url = `https://api.github.com/repos/${GITHUB_REPO}/contents/keywords_full.json`;
                  const headers = { 'Authorization': `token ${GITHUB_TOKEN}`, 'Content-Type': 'application/json' };
                  const content = JSON.stringify(keywordsFullData, null, 2);
                  const encodedContent = Buffer.from(content).toString('base64');
                  const body = { message: `æ›´æ–°å…¨æ™¯å…³é”®è¯ - ${new Date().toISOString()}`, content: encodedContent, branch: 'main' };
                  try {
                    const response = await fetch(url, { headers: { 'Authorization': `token ${GITHUB_TOKEN}` } });
                    if (response.ok) { body.sha = (await response.json()).sha; }
                  } catch (error) { console.log("keywords_full.json not found. Creating a new one."); }
                  const response = await fetch(url, { method: 'PUT', headers, body: JSON.stringify(body) });
                  if (!response.ok) { throw new Error(`ä¿å­˜å…¨æ™¯å…³é”®è¯å¤±è´¥ (${response.status}): ${await response.text()}`); }
                  console.log("âœ… æˆåŠŸä¿å­˜å…¨æ™¯å…³é”®è¯åˆ° keywords_full.json");
                } catch (error) { console.error("âŒ ä¿å­˜å…¨æ™¯å…³é”®è¯æ—¶å‡ºé”™:", error); }
              }
              
              if (sortedKeywordsFull.length > 0) {
                  await saveKeywordsFullToGitHub(sortedKeywordsFull);
              }

              // --- ä¿®æ”¹ï¼šæ›´æ–°çƒ­é—¨å…³é”®è¯ï¼Œå¹¶ä¿æŒæ‰‹åŠ¨å…³é”®è¯ä¸å˜ ---
              const TOP_N_HOT = 3; 
              const newHotKeywords = sortedKeywordsFull.slice(0, TOP_N_HOT).map(item => item.keyword);
              console.log(`æ–°çš„çƒ­é—¨å…³é”®è¯ä¸º: ${newHotKeywords.join(', ')}`);
              // --- ä¿®æ”¹ç»“æŸ ---
              
              // --- æ­¥éª¤ 5 & 6: åˆå¹¶å¹¶æˆªå–æœ€ç»ˆçš„è®ºæ–‡åˆ—è¡¨ ---
              const combinedPapers = [...newUniquePapers, ...(existingData.papers || [])];
              const MAX_PAPERS_TO_KEEP = 50; // å®šä¹‰æœ€å¤šä¿ç•™å¤šå°‘ç¯‡è®ºæ–‡
              const finalPapers = combinedPapers.slice(0, MAX_PAPERS_TO_KEEP);

              console.log(`å°†ä¿ç•™æœ€æ–°çš„ ${finalPapers.length} ç¯‡è®ºæ–‡ã€‚`);

              // --- æ­¥éª¤ 7: å‡†å¤‡å¹¶ä¿å­˜æœ€ç»ˆæ•°æ® ---
              // --- æ­¥éª¤ 7: å‡†å¤‡å¹¶ä¿å­˜æœ€ç»ˆæ•°æ® ---
              const newData = {
                papers: finalPapers, // ä½¿ç”¨åˆå¹¶ã€å»é‡ã€æˆªå–åçš„æœ€ç»ˆåˆ—è¡¨
                lastUpdate: getBeijingTime().toISOString(),
                // --- æ–°å¢ï¼šä¿å­˜æ–°çš„å…³é”®è¯å¯¹è±¡ç»“æ„ ---
                keywords: {
                  manual: manualKeywords, // æ‰‹åŠ¨å…³é”®è¯åˆ—è¡¨ä¿æŒä¸å˜
                  hot: newHotKeywords      // çƒ­é—¨å…³é”®è¯åˆ—è¡¨è¢«å®Œå…¨æ›¿æ¢
                },
                // --- ä¿®æ”¹ç»“æŸ ---
                lastAutoUpdateDate: getBeijingTime().toLocaleDateString('zh-CN', {timeZone: 'Asia/Shanghai'})
              };
              
              await saveToGitHub(newData, existingData.sha);
              console.log(`æˆåŠŸå¤„ç†äº† ${newUniquePapers.length} ç¯‡æ–°è®ºæ–‡`);
              
            } catch (error) {
              console.error("æ›´æ–°è¿‡ç¨‹ä¸­å‡ºé”™:", error);
              process.exit(1);
            }
          }
          main();
          EOL

      - name: Run updater
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
        run: node update-data.js


      - name: Send email notification
        if: success()
        uses: dawidd6/action-send-mail@v5
        with:
          server_address: smtp.163.com
          server_port: 465
          secure: true
          username: 13937372851@163.com
          password: PCf7BnBBtXx9c6wk
          subject: å­¦æœ¯å‘¨æŠ¥å·²æ›´æ–° - ${{ github.repository }}
          body: |
            æ‚¨å¥½ï¼Œ
            
            æ‚¨çš„å­¦æœ¯å‘¨æŠ¥å·²æˆåŠŸæ›´æ–°ï¼æ–°çš„å­¦æœ¯è®ºæ–‡å·²ç»æ·»åŠ åˆ°æ•°æ®åº“ä¸­ã€‚
            
            æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹é“¾æ¥è®¿é—®æ‚¨çš„å­¦æœ¯å‘¨æŠ¥ï¼š
            https://jannhsu.github.io/paper_summary/
            
            æ›´æ–°æ—¶é—´ï¼š${{ github.event.repository.updated_at }}
            ä»“åº“ï¼š${{ github.repository }}
            
            æ­¤é‚®ä»¶ç”±GitHub Actionsè‡ªåŠ¨å‘é€ï¼Œè¯·å‹¿å›å¤ã€‚
          to: blumanchu111@gmail.com
          from: å­¦æœ¯å‘¨æŠ¥è‡ªåŠ¨æ›´æ–° <13937372851@163.com>
          nodemailerlog: true
          nodemailerdebug: true